{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Install Facebook API SDK\n",
    "\n",
    "#! pip install -e git+https://github.com/mobolic/facebook-sdk.git#egg=facebook-sdk\n",
    "\n",
    "# If above does not work, try cutting & pasting into command prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import facebook\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Set Up Graph Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate your temporary token at https://developers.facebook.com/tools/explorer/ and paste below (Needs a Facebook account)\n",
    "\n",
    "temporary_token = ''\n",
    "graph = facebook.GraphAPI(access_token=temporary_token, version='2.9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Article functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get formatted IDs for use in API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_fb_id(pub_id, post_id):\n",
    "    '''\n",
    "    Return a well-formatted Facebook ID for posts, to be used in other functions.\n",
    "\n",
    "    Args:\n",
    "        pub_id: string or int for the publisher ID on Facebook\n",
    "        post_id: string or int for the publisher ID on Facebook\n",
    "    \n",
    "    Returns:\n",
    "        post: Facebook ID in the format of \"'pub_id'_'post_id'\"\n",
    "    '''\n",
    "    # Assure IDs are strings or ints\n",
    "    try:\n",
    "        isinstance(int(pub_id), int) or isinstance(pub_id, str)\n",
    "        isinstance(int(post_id), int) or isinstance(post_id, str)\n",
    "    except:\n",
    "        print('Input error: Facebook ID must be an integer or a string integer.')\n",
    "        return\n",
    "    \n",
    "    # Convert non-string IDs to strings\n",
    "    if not isinstance(pub_id, str):\n",
    "        pub_id = str(pub_id)\n",
    "    \n",
    "    if not isinstance(post_id, str):\n",
    "        post_id = str(post_id)    \n",
    "    \n",
    "    fb_id = \"_\".join([pub_id, post_id])\n",
    "    \n",
    "    return fb_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get News Story from Publisher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_article(fb_id):\n",
    "    '''\n",
    "    Return the news story article object, defined by the Publisher ID and Post ID.\n",
    "    \n",
    "    Args:\n",
    "        fb_id: string ID composed of publisher and post ID, as returned by get_fb_id()\n",
    "    \n",
    "    Returns:\n",
    "        article: dict object\n",
    "    '''\n",
    "    # Get post from API (returns dict)\n",
    "    args = {'fields' : 'id, message, created_time, permalink_url, type, updated_time'}\n",
    "    article = graph.get_object(id=fb_id,  **args)\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add total counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def article_stats(fb_id, article=dict()):\n",
    "    '''\n",
    "    Get total counts of likes, shares and comments, return dict with these fields.\n",
    "    \n",
    "    If the second input argument is passed (as dict), these fields are added to the input ('article').\n",
    "    '''\n",
    "    # Assure that 'article' is dict\n",
    "    try:\n",
    "        is_dict = isinstance(article, dict)\n",
    "        if not is_dict:\n",
    "            raise TypeError('Not dict')\n",
    "    except TypeError:\n",
    "        print('Input error: Second argument, if specified, must be a dictonary.')\n",
    "        return\n",
    "    \n",
    "    # Likes\n",
    "    obj = graph.get_connections(id=fb_id, connection_name='likes', summary=True, limit=0)\n",
    "    article['likes_count'] = obj['summary']['total_count']\n",
    "\n",
    "    # Shares\n",
    "    obj = graph.get_object(id=fb_id, fields='shares')\n",
    "    article['shares_count'] = obj['shares']['count']\n",
    "    \n",
    "    # Comments\n",
    "    obj = graph.get_connections(id=fb_id, connection_name='comments', summary=True, limit=0)\n",
    "    article['comments_count'] = obj['summary']['total_count']\n",
    "    \n",
    "    return article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Article data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_article_json(fb_id, article, topic, publisher):\n",
    "    '''\n",
    "    Save Article data in JSON format into file named 'topic_publisher_articles.json'\n",
    "    '''\n",
    "    # Add Topic and Publisher metadata to article object\n",
    "    article['topic'] = topic\n",
    "    article['publisher'] = publisher\n",
    "    \n",
    "    # Add Reactions and Comments files to metadata\n",
    "    article['reactions_file'] = '{}_{}_reactions.csv'.format(topic,publisher)\n",
    "    article['comments_file'] = '{}_{}_comments.json'.format(topic,publisher)\n",
    "    \n",
    "    # Save data in JSON format to disk\n",
    "    with open('{}_{}_article.json'.format(topic,publisher), 'w') as f:\n",
    "        json.dump(article, f, ensure_ascii=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and save Article data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_data(fb_id, topic, publisher):\n",
    "    '''\n",
    "    Get and save Article data into JSON file.\n",
    "    \n",
    "    Has the following keys:\n",
    "    'id', 'message', 'created_time', 'permalink_url', 'type', 'updated_time', \n",
    "    'likes_count', 'shares_count', 'comments_count', \n",
    "    'topic', 'publisher', 'reactions_file', 'comments_file'\n",
    "    \n",
    "    If you are having trouble reading the file, try pasting the content here: https://jsonlint.com/\n",
    "    '''\n",
    "    # Fetch article object\n",
    "    article = get_article(fb_id)\n",
    "    \n",
    "    # Add stats\n",
    "    article_stats(fb_id, article);\n",
    "    \n",
    "    # Print Article info\n",
    "    print('  Processed article metadata')\n",
    "\n",
    "    # Add metadata and save in JSON format to disk\n",
    "    save_article_json(fb_id, article, topic, publisher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Reaction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reactions(fb_id):\n",
    "    '''\n",
    "    Get set of ID, name and type of reaction for each user (in dict), return list of dicts.\n",
    "    '''\n",
    "    # Step through pagination of results\n",
    "    page_lim = 1000\n",
    "    reactions = list()\n",
    "    paging_after = ''\n",
    "    more_data = True\n",
    "    \n",
    "    while more_data:\n",
    "        obj = graph.get_connections(id=fb_id, connection_name='reactions', limit=page_lim, after=paging_after)\n",
    "        if obj['data']:\n",
    "            reactions.extend(obj['data'])\n",
    "            paging_after = obj['paging']['cursors']['after']\n",
    "        else:\n",
    "            more_data = False\n",
    "    \n",
    "    return reactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_reactions_csv(fb_id, reactions, topic, publisher):\n",
    "    '''\n",
    "    Save Reactions data in CSV format into file named 'topic_publisher_reactions.csv'\n",
    "    '''\n",
    "    \n",
    "    # ToDo: Some names give errors, suspect encoding problem. If time and possible, feel free to fix name.\n",
    "    #       For now, have excluded name (and possible gender classification) from Reactions.\n",
    "    list_react = [[fb_id, d['id'], d['type']] for d in reactions]\n",
    "    #list_react = [[d['id'], d['name'], d['type']] for d in reactions]\n",
    "    \n",
    "    # Save Reactions in CSV format to disk\n",
    "    with open('{}_{}_reactions.csv'.format(topic,publisher), 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['parent_id','id','type'])\n",
    "        writer.writerows(list_react)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_reactions_data(fb_id, topic, publisher):\n",
    "    '''\n",
    "    Get and save Reactions to a news article in a CSV file.\n",
    "    \n",
    "    Has the following headers:\n",
    "    'parent_id': ID of the article\n",
    "    'id': Anonymised user ID\n",
    "    'type': Kind of reaction. One of LIKE, LOVE, HAHA, WOW, SAD, ANGRY\n",
    "    '''\n",
    "    # Fetch article object\n",
    "    reactions = get_reactions(fb_id)\n",
    "    \n",
    "    # Print number of reactions\n",
    "    print('  Processed {} reactions'.format(len(reactions)))\n",
    "    \n",
    "    # Save data in CSV format to disk\n",
    "    save_reactions_csv(fb_id, reactions, topic, publisher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Comment functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_comments(fb_id):\n",
    "    '''\n",
    "    Steps through and retreives comments and replies (only first level replies). Returns list of dicts.\n",
    "    '''\n",
    "    # Step through pagination of results\n",
    "    page_lim = 1000\n",
    "    comments = list()\n",
    "    paging_after = ''\n",
    "    more_data = True\n",
    "\n",
    "    # Loop through root comments one-by-one and get their replies\n",
    "    while more_data:\n",
    "        args = {'fields' : 'id, created_time, from, message, comment_count, like_count'}\n",
    "        obj_parent = graph.get_connections(id=fb_id, connection_name='comments', limit=1, after=paging_after, **args)\n",
    "        \n",
    "        if obj_parent['data']:\n",
    "            # Add root comment and add fields to improve data handling\n",
    "            comments.extend(obj_parent['data'])\n",
    "            comments[-1]['parent_id'] = fb_id\n",
    "            comments[-1]['type'] = 'root'\n",
    "            \n",
    "            # Add list of likes of root comment (if any)\n",
    "            obj_parent_likes = graph.get_connections(id=obj_parent['data'][0]['id'], connection_name='likes', limit=10000)\n",
    "            if obj_parent['data'][0]['like_count']:\n",
    "                comments[-1]['likes'] = obj_parent_likes['data']\n",
    "            else:\n",
    "                comments[-1]['likes'] = []\n",
    "            \n",
    "            # Add replies to root comment (if any) and add fields to help data handling\n",
    "            if obj_parent['data'][0]['comment_count']:\n",
    "                obj_child = graph.get_connections(id=obj_parent['data'][0]['id'], connection_name='comments', limit=10000, after='', **args)\n",
    "                for d in obj_child['data']:\n",
    "                    d.update({'parent_id':obj_parent['data'][0]['id'], 'type':'reply'})\n",
    "                    obj_child_likes = graph.get_connections(id=d['id'], connection_name='likes', limit=10000)\n",
    "                    if obj_child['data'][0]['like_count']:\n",
    "                        d.update({'likes':obj_child_likes['data']})\n",
    "                    else:\n",
    "                        d.update({'likes':[]})\n",
    "                                  \n",
    "                comments.extend(obj_child['data'])\n",
    "\n",
    "            # Get next root object pointer\n",
    "            paging_after = obj_parent['paging']['cursors']['after']\n",
    "        else:\n",
    "            more_data = False\n",
    "    \n",
    "    return comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_comments_json(fb_id, comments, topic, publisher):\n",
    "    '''\n",
    "    Save Comments in JSON format into file named 'topic_publisher_comments.json'\n",
    "    '''\n",
    "    # Save data in JSON format to disk\n",
    "    with open('{}_{}_comments.json'.format(topic,publisher), 'w') as f:\n",
    "        json.dump(comments, f, ensure_ascii=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_comments_data(fb_id, topic, publisher):\n",
    "    '''\n",
    "    Get and save Reactions to a news article in a CSV file.\n",
    "    \n",
    "    Has the following headers:\n",
    "    'parent_id': ID of the article\n",
    "    'id': Anonymised user ID\n",
    "    'type': Kind of reaction. One of LIKE, LOVE, HAHA, WOW, SAD, ANGRY\n",
    "    '''\n",
    "    # Fetch article object\n",
    "    comments = get_comments(fb_id)\n",
    "    \n",
    "    # Print number of comments\n",
    "    print('  Processed {} comments'.format(len(comments)))\n",
    "\n",
    "    # Save data in CSV format to disk\n",
    "    save_comments_json(fb_id, comments, topic, publisher)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterate through list of news articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_news_stories(filename):\n",
    "    \n",
    "    articles = list()\n",
    "    \n",
    "    with open(filename) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for row in reader:\n",
    "            articles.append(row)\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1 of 44: Grenfell HuffingtonPost\n",
      "  Processed article metadata\n",
      "  Processed 9883 reactions\n",
      "  Processed 458 comments\n",
      "Processing file 2 of 44: Grenfell DailyMail\n",
      "  Processed article metadata\n",
      "  Processed 6118 reactions\n",
      "  Processed 501 comments\n",
      "Processing file 3 of 44: Grenfell Standard\n",
      "  Processed article metadata\n",
      "  Processed 37969 reactions\n",
      "  Processed 573 comments\n",
      "Processing file 4 of 44: Grenfell Guardian\n",
      "  Processed article metadata\n",
      "  Processed 28298 reactions\n",
      "  Processed 1374 comments\n",
      "Processing file 5 of 44: Grenfell Independent\n",
      "  Processed article metadata\n",
      "  Processed 24522 reactions\n",
      "  Processed 1419 comments\n",
      "Processing file 6 of 44: Grenfell Telegraph\n",
      "  Processed article metadata\n",
      "  Processed 16900 reactions\n",
      "  Processed 699 comments\n",
      "Processing file 7 of 44: Grenfell TheSun\n",
      "  Processed article metadata\n",
      "  Processed 2365 reactions\n",
      "  Processed 403 comments\n",
      "Processing file 8 of 44: Grenfell BBC\n",
      "  Processed article metadata\n",
      "  Processed 21939 reactions\n",
      "  Processed 874 comments\n",
      "Processing file 9 of 44: A50_Commons EveningStandard\n",
      "  Processed article metadata\n",
      "  Processed 1388 reactions\n",
      "  Processed 320 comments\n",
      "Processing file 10 of 44: A50_Commons FinancialTimes\n",
      "  Processed article metadata\n",
      "  Processed 1167 reactions\n",
      "  Processed 114 comments\n",
      "Processing file 11 of 44: A50_Commons Sun\n",
      "  Processed article metadata\n",
      "  Processed 3710 reactions\n"
     ]
    }
   ],
   "source": [
    "articles = read_news_stories('topic_publisher_list.csv')\n",
    "\n",
    "# Remove first line containing headers\n",
    "articles = articles[1:]\n",
    "\n",
    "for i, row in enumerate(articles):\n",
    "    publisher, topic, pub_id, post_id, permalink_url = row\n",
    "   \n",
    "    fb_id = get_fb_id(pub_id, post_id)\n",
    "\n",
    "    # Get data!\n",
    "    print('Processing file {} of {}: {} {}'.format(i+1, len(articles), topic, publisher))\n",
    "\n",
    "    get_article_data(fb_id, topic, publisher)\n",
    "    get_reactions_data(fb_id, topic, publisher)\n",
    "    get_comments_data(fb_id, topic, publisher)\n",
    "        \n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
